#!/usr/bin/env python3
"""
MutCompute Use Case 3: Mutation Hotspot Analysis

This script identifies mutation hotspots in a protein structure by analyzing
the mutation probability matrix generated by MutCompute. It highlights residues
that are most/least tolerant to mutations and provides insights for protein
engineering and stability analysis.

Usage:
    python use_case_3_mutation_hotspot_analysis.py --input examples/data/1y4a_BPN_mutcompute.csv
    python use_case_3_mutation_hotspot_analysis.py --input predictions.csv --output hotspot_analysis.csv
"""

import sys
import argparse
import numpy as np
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_mutation_tolerance(df):
    """Analyze mutation tolerance for each residue."""
    # Calculate mutation tolerance metrics
    tolerance_data = []

    amino_acids = ['ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLN', 'GLU', 'GLY', 'HIS', 'ILE',
                   'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL']

    prob_columns = [f'pr{aa}' for aa in amino_acids]

    for _, row in df.iterrows():
        residue_probs = row[prob_columns].values

        # Calculate various tolerance metrics
        wt_aa = row['wtAA']
        wt_prob = row['wt_prob']
        pred_prob = row['pred_prob']

        # Shannon entropy - higher = more tolerance to mutations
        residue_probs_array = np.array(residue_probs, dtype=np.float64)
        entropy = -np.sum(residue_probs_array * np.log2(np.maximum(residue_probs_array, 1e-10)))

        # Maximum non-wild-type probability
        non_wt_probs = residue_probs_array.copy()
        wt_index = amino_acids.index(wt_aa)
        non_wt_probs[wt_index] = 0
        max_alternative_prob = np.max(non_wt_probs)
        max_alternative_aa = amino_acids[np.argmax(non_wt_probs)]

        # Probability mass in top 3 alternatives (excluding wild-type)
        top3_alternatives = np.sort(non_wt_probs)[-3:].sum()

        # Conservation score (1 - entropy / max_entropy)
        max_entropy = np.log2(20)  # Maximum possible entropy for 20 amino acids
        conservation_score = 1 - (entropy / max_entropy)

        # Mutation sensitivity (how much probability is concentrated in WT)
        mutation_sensitivity = wt_prob

        tolerance_data.append({
            'aa_id': row['aa_id'],
            'pdb_id': row['pdb_id'],
            'chain_id': row['chain_id'],
            'pos': row['pos'],
            'wtAA': wt_aa,
            'wt_prob': wt_prob,
            'pred_prob': pred_prob,
            'avg_log_ratio': row['avg_log_ratio'],
            'entropy': entropy,
            'conservation_score': conservation_score,
            'mutation_sensitivity': mutation_sensitivity,
            'max_alternative_prob': max_alternative_prob,
            'max_alternative_aa': max_alternative_aa,
            'top3_alternatives_prob': top3_alternatives
        })

    return pd.DataFrame(tolerance_data)

def identify_hotspots(tolerance_df, percentile_threshold=10):
    """Identify mutation hotspots based on multiple criteria."""
    # Define hotspots as residues that are:
    # 1. Highly conserved (low entropy, high conservation score)
    # 2. Sensitive to mutations (high mutation sensitivity)
    # 3. Have low tolerance for alternatives

    # Calculate percentile thresholds
    low_entropy_threshold = np.percentile(tolerance_df['entropy'], percentile_threshold)
    high_conservation_threshold = np.percentile(tolerance_df['conservation_score'], 100 - percentile_threshold)
    high_sensitivity_threshold = np.percentile(tolerance_df['mutation_sensitivity'], 100 - percentile_threshold)

    # Identify different types of positions
    tolerance_df['is_conserved'] = tolerance_df['entropy'] <= low_entropy_threshold
    tolerance_df['is_highly_conserved'] = tolerance_df['conservation_score'] >= high_conservation_threshold
    tolerance_df['is_mutation_sensitive'] = tolerance_df['mutation_sensitivity'] >= high_sensitivity_threshold

    # Combine criteria for hotspot identification
    tolerance_df['hotspot_score'] = (
        tolerance_df['is_conserved'].astype(int) +
        tolerance_df['is_highly_conserved'].astype(int) +
        tolerance_df['is_mutation_sensitive'].astype(int)
    )

    tolerance_df['is_hotspot'] = tolerance_df['hotspot_score'] >= 2

    # Identify flexible positions (high tolerance to mutations)
    high_entropy_threshold = np.percentile(tolerance_df['entropy'], 100 - percentile_threshold)
    tolerance_df['is_flexible'] = tolerance_df['entropy'] >= high_entropy_threshold

    return tolerance_df

def generate_summary_report(tolerance_df, output_dir=None):
    """Generate a comprehensive summary report."""
    total_residues = len(tolerance_df)
    hotspots = tolerance_df[tolerance_df['is_hotspot']]
    flexible_positions = tolerance_df[tolerance_df['is_flexible']]

    report = []
    report.append("=== MUTATION HOTSPOT ANALYSIS REPORT ===\n")

    report.append(f"Total residues analyzed: {total_residues}")
    report.append(f"Mutation hotspots identified: {len(hotspots)} ({len(hotspots)/total_residues*100:.1f}%)")
    report.append(f"Flexible positions identified: {len(flexible_positions)} ({len(flexible_positions)/total_residues*100:.1f}%)\n")

    # Overall statistics
    report.append("=== OVERALL STATISTICS ===")
    report.append(f"Average entropy: {tolerance_df['entropy'].mean():.3f} ± {tolerance_df['entropy'].std():.3f}")
    report.append(f"Average conservation score: {tolerance_df['conservation_score'].mean():.3f} ± {tolerance_df['conservation_score'].std():.3f}")
    report.append(f"Average wild-type probability: {tolerance_df['wt_prob'].mean():.3f} ± {tolerance_df['wt_prob'].std():.3f}")
    report.append(f"Average log ratio: {tolerance_df['avg_log_ratio'].mean():.3f} ± {tolerance_df['avg_log_ratio'].std():.3f}\n")

    # Hotspot details
    if len(hotspots) > 0:
        report.append("=== MUTATION HOTSPOTS ===")
        report.append("(Residues with low mutation tolerance)")
        hotspots_sorted = hotspots.sort_values('hotspot_score', ascending=False)
        for _, row in hotspots_sorted.head(10).iterrows():
            report.append(f"  {row['aa_id']}: {row['wtAA']}{row['pos']} (score: {row['hotspot_score']}, conservation: {row['conservation_score']:.3f})")

    # Flexible positions
    if len(flexible_positions) > 0:
        report.append("\n=== FLEXIBLE POSITIONS ===")
        report.append("(Residues with high mutation tolerance)")
        flexible_sorted = flexible_positions.sort_values('entropy', ascending=False)
        for _, row in flexible_sorted.head(10).iterrows():
            report.append(f"  {row['aa_id']}: {row['wtAA']}{row['pos']} (entropy: {row['entropy']:.3f}, best alt: {row['max_alternative_aa']})")

    # Chain-wise analysis
    if 'chain_id' in tolerance_df.columns:
        report.append("\n=== CHAIN-WISE ANALYSIS ===")
        chain_stats = tolerance_df.groupby('chain_id').agg({
            'is_hotspot': 'sum',
            'is_flexible': 'sum',
            'entropy': 'mean',
            'conservation_score': 'mean'
        }).round(3)

        for chain_id, stats in chain_stats.iterrows():
            chain_total = len(tolerance_df[tolerance_df['chain_id'] == chain_id])
            report.append(f"  Chain {chain_id}: {chain_total} residues, {stats['is_hotspot']} hotspots, {stats['is_flexible']} flexible")

    report_text = '\n'.join(report)

    if output_dir:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        report_file = output_dir / "hotspot_analysis_report.txt"
        with open(report_file, 'w') as f:
            f.write(report_text)
        print(f"Report saved to: {report_file}")

    return report_text

def create_visualizations(tolerance_df, output_dir=None):
    """Create visualizations of the mutation analysis."""
    if output_dir:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

    # 1. Entropy vs Conservation scatter plot
    plt.figure(figsize=(10, 6))
    scatter = plt.scatter(tolerance_df['entropy'], tolerance_df['conservation_score'],
                         c=tolerance_df['hotspot_score'], cmap='viridis', alpha=0.6)
    plt.xlabel('Entropy (Mutation Tolerance)')
    plt.ylabel('Conservation Score')
    plt.title('Mutation Tolerance vs Conservation')
    plt.colorbar(scatter, label='Hotspot Score')

    if output_dir:
        plt.savefig(output_dir / 'entropy_vs_conservation.png', dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

    # 2. Position-wise hotspot visualization
    plt.figure(figsize=(12, 6))
    colors = ['blue' if x else 'gray' for x in tolerance_df['is_hotspot']]
    plt.scatter(tolerance_df['pos'], tolerance_df['conservation_score'], c=colors, alpha=0.6)
    plt.xlabel('Residue Position')
    plt.ylabel('Conservation Score')
    plt.title('Conservation Score by Position (Blue = Hotspots)')

    if output_dir:
        plt.savefig(output_dir / 'position_conservation.png', dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

    # 3. Entropy distribution
    plt.figure(figsize=(10, 6))
    plt.hist(tolerance_df['entropy'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    plt.axvline(tolerance_df['entropy'].mean(), color='red', linestyle='--', label=f'Mean: {tolerance_df["entropy"].mean():.3f}')
    plt.xlabel('Entropy')
    plt.ylabel('Frequency')
    plt.title('Distribution of Mutation Tolerance (Entropy)')
    plt.legend()

    if output_dir:
        plt.savefig(output_dir / 'entropy_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

def main():
    parser = argparse.ArgumentParser(
        description='Analyze mutation hotspots from MutCompute predictions',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Analyze predictions from MutCompute output
    python use_case_3_mutation_hotspot_analysis.py --input examples/data/1y4a_BPN_mutcompute.csv

    # Save detailed analysis to files
    python use_case_3_mutation_hotspot_analysis.py --input predictions.csv --output_dir hotspot_analysis

    # Adjust sensitivity for hotspot detection
    python use_case_3_mutation_hotspot_analysis.py --input predictions.csv --percentile 5

Output includes:
    - Detailed tolerance analysis CSV
    - Summary report with hotspot identification
    - Visualization plots (if output directory specified)
        """
    )

    parser.add_argument('--input', '-i', required=True,
                        help='Input CSV file from MutCompute predictions')

    parser.add_argument('--output_dir', '-o',
                        help='Output directory for analysis files and plots')

    parser.add_argument('--percentile', '-p', type=float, default=10,
                        help='Percentile threshold for hotspot identification (default: 10)')

    args = parser.parse_args()

    try:
        # Load MutCompute predictions
        input_file = Path(args.input)
        if not input_file.exists():
            raise FileNotFoundError(f"Input file not found: {input_file}")

        print(f"Loading MutCompute predictions from: {input_file}")
        df = pd.read_csv(input_file)

        print(f"Analyzing {len(df)} residues...")

        # Perform tolerance analysis
        tolerance_df = analyze_mutation_tolerance(df)

        # Identify hotspots
        tolerance_df = identify_hotspots(tolerance_df, args.percentile)

        # Save detailed analysis
        if args.output_dir:
            output_dir = Path(args.output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
            output_file = output_dir / "tolerance_analysis.csv"
            tolerance_df.to_csv(output_file, index=False)
            print(f"Detailed analysis saved to: {output_file}")

        # Generate summary report
        report = generate_summary_report(tolerance_df, args.output_dir)
        print("\n" + report)

        # Create visualizations
        if args.output_dir:
            print("\nGenerating visualizations...")
            create_visualizations(tolerance_df, args.output_dir)
            print(f"Visualizations saved to: {args.output_dir}")

        # Print key findings
        hotspots = tolerance_df[tolerance_df['is_hotspot']]
        flexible = tolerance_df[tolerance_df['is_flexible']]

        print(f"\n=== KEY FINDINGS ===")
        print(f"Identified {len(hotspots)} mutation hotspots and {len(flexible)} flexible positions")

        if len(hotspots) > 0:
            most_critical = hotspots.loc[hotspots['conservation_score'].idxmax()]
            print(f"Most critical hotspot: {most_critical['wtAA']}{most_critical['pos']} (conservation: {most_critical['conservation_score']:.3f})")

        if len(flexible) > 0:
            most_flexible = flexible.loc[flexible['entropy'].idxmax()]
            print(f"Most flexible position: {most_flexible['wtAA']}{most_flexible['pos']} (entropy: {most_flexible['entropy']:.3f})")

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()